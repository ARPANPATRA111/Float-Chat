In the Quickconnect bar at the top:
Host: ftp.ifremer.fr
Username: anonymous
Password: your_email@example.com
Port: 21
Click Quickconnect.

---

# Virtual Envirnment activation Script 
cd net
.venv\Scripts\activate

---

# Make sure ollama is running
1) open cmd and run = ollama run llama3.2:latest
- to check what are the llm present locally run = ollama list

--- 

# To Truncate the posgress-DB
1) open psql shell in start menu
2) press enter until it asks for the password 
3) enter password = 123123
4) copy-paste this = \c argo_db
5) copy-paste this = TRUNCATE TABLE argo_profiles, float_metadata, spatial_ref_sys RESTART IDENTITY;
6) \q and press enter

---






# --- data_processing_verbose.py ---
import os
import glob
import xarray as xr
import pandas as pd
import numpy as np
import logging
import traceback # Used for printing detailed error messages

# Ensure all necessary modules can be imported
try:
    from database_manager import DatabaseManager
    from config import DATA_PROCESSING_CONFIG, USE_SQLITE
    from rag_system import VectorStoreManager
except ImportError as e:
    print(f"FATAL ERROR: Could not import necessary modules. Please check your project structure. Details: {e}")
    exit()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ArgoDataProcessor:
    def __init__(self, data_dir=None):
        print("-> Initializing ArgoDataProcessor...")
        self.data_dir = data_dir or DATA_PROCESSING_CONFIG["data_dir"]
        self.db_manager = DatabaseManager()
        self.vector_store = VectorStoreManager()
        print("-> Processor initialized successfully.")
    
    def _convert_time(self, val):
        try:
            if np.issubdtype(type(val), np.number) and not np.isnan(val):
                return pd.to_datetime("1950-01-01") + pd.to_timedelta(float(val), unit="D")
        except Exception:
            return pd.NaT
        return pd.NaT

    def _extract_metadata(self, ds):
        try:
            float_id = ds['PLATFORM_NUMBER'].values[0].decode('utf-8').strip()
            params = [var for var in ds.data_vars if '_ADJUSTED' in var]
            
            metadata = {
                "float_id": float_id,
                "wmo_id": ds['WMO_INST_TYPE'].values[0].decode('utf-8').strip(),
                "project_name": ds['PROJECT_NAME'].values[0].decode('utf-8').strip(),
                "institution": ds.attrs.get('institution', 'Unknown'),
                "date_launched": str(ds.get('LAUNCH_DATE', 'N/A')),
                "parameters": params
            }
            return metadata
        except Exception as e:
            # Verbose: Print why metadata extraction failed
            print(f"--> ERROR: Could not extract metadata. Details: {e}")
            return None

    def _generate_metadata_summary(self, metadata):
        if not metadata: return None
        summary = (
            f"ARGO float ID {metadata['float_id']} from the {metadata['project_name']} project, "
            f"managed by {metadata['institution']}. It measures parameters including: "
            f"{', '.join(metadata['parameters'])}."
        )
        return summary

    def process_netcdf_file(self, file_path):
        try:
            with xr.open_dataset(file_path, decode_times=False) as ds:
                file_metadata = self._extract_metadata(ds)
                if not file_metadata:
                    print("--> Metadata extraction failed. Skipping file.")
                    return None, None

                records = []
                num_profiles = ds.dims['N_PROF']
                # Verbose: Print how many profiles were found in the file
                print(f"--> Found {num_profiles} profiles in this file.")

                for i in range(num_profiles):
                    profile_data = ds.isel(N_PROF=i)
                    
                    profile_time = self._convert_time(profile_data['JULD'].values)
                    profile_lat = profile_data['LATITUDE'].values
                    profile_lon = profile_data['LONGITUDE'].values
                    cycle_num = int(profile_data['CYCLE_NUMBER'].values)

                    depths = profile_data['PRES_ADJUSTED'].values
                    temps = profile_data['TEMP_ADJUSTED'].values
                    sals = profile_data['PSAL_ADJUSTED'].values
                    
                    for j in range(len(depths)):
                        if not np.isnan(depths[j]) and not np.isnan(temps[j]) and not np.isnan(sals[j]):
                            records.append({
                                'float_id': file_metadata['float_id'],
                                'profile_number': cycle_num,
                                'time': profile_time,
                                'lat': float(profile_lat),
                                'lon': float(profile_lon),
                                'depth': float(depths[j]),
                                'temperature': float(temps[j]),
                                'salinity': float(sals[j]),
                                'doxy': float(profile_data.get('DOXY_ADJUSTED', [np.nan])[j]) if 'DOXY_ADJUSTED' in profile_data else None,
                                'chla': float(profile_data.get('CHLA_ADJUSTED', [np.nan])[j]) if 'CHLA_ADJUSTED' in profile_data else None
                            })
                
                if not records:
                    print("--> No valid measurement records found after processing all profiles.")
                    return None, None
                
                # Verbose: Print how many valid records were compiled
                print(f"--> Compiled {len(records)} valid measurement records from this file.")
                return pd.DataFrame(records), file_metadata

        except Exception as e:
            print(f"--> FATAL ERROR processing file {os.path.basename(file_path)}. Details: {e}")
            traceback.print_exc()
            return None, None

    def process_directory(self, max_files=None):
        print("\n-> Starting directory processing...")
        try:
            self.db_manager.initialize_database()
            print("-> Database connection successful and tables are ready.")
        except Exception as e:
            print(f"-> FATAL ERROR: Could not initialize database. Is PostgreSQL running and configured correctly?")
            print(f"-> Details: {e}")
            traceback.print_exc()
            return

        nc_files_path = os.path.join(self.data_dir, "*.nc")
        print(f"-> Searching for NetCDF files in: {os.path.abspath(nc_files_path)}")
        
        nc_files = sorted(glob.glob(nc_files_path))
        print(f"-> Found {len(nc_files)} NetCDF files.")
        
        if not nc_files:
            print("-> WARNING: No files found. Please ensure .nc files are in the 'data' directory.")
            return

        if max_files: 
            nc_files = nc_files[:max_files]
            print(f"-> Will process a maximum of {len(nc_files)} files.")
        
        processed_count = 0
        for file_path in nc_files:
            print(f"\n-> Processing file: {os.path.basename(file_path)}")
            df, metadata = self.process_netcdf_file(file_path)
            
            if df is not None and not df.empty and metadata:
                if not USE_SQLITE:
                    df['geom'] = df.apply(lambda row: f'SRID=4326;POINT({row.lon} {row.lat})', axis=1)

                success = self.db_manager.insert_argo_data(df, metadata)
                
                if success:
                    print(f"--> SUCCESS: Inserted {len(df)} records into SQL database.")
                    summary_text = self._generate_metadata_summary(metadata)
                    self.vector_store.add_document(
                        doc_id=metadata['float_id'],
                        document=summary_text,
                        metadata={"float_id": metadata['float_id']}
                    )
                    print(f"--> SUCCESS: Added/updated metadata for float {metadata['float_id']} in vector store.")
                    processed_count += 1
                else:
                    print(f"--> FAILED to insert data into SQL database.")
            else:
                print("--> SKIPPED: No valid data could be extracted from this file.")
        
        print(f"\n-> Processing complete. Successfully processed {processed_count}/{len(nc_files)} files.")

if __name__ == "__main__":
    print("=============================================")
    print("   Running ARGO Data Processor in Verbose Mode   ")
    print("=============================================")
    try:
        processor = ArgoDataProcessor()
        processor.process_directory(max_files=DATA_PROCESSING_CONFIG.get('max_files'))
    except Exception as e:
        print(f"\n-> A critical error occurred during script execution.")
        print(f"-> Details: {e}")
        traceback.print_exc()